{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数的访问，初始化和共享\n",
    "#### 在线性回归的简洁实现中，我们通过init模块来初始化模型的参数，我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数\n",
    "#### 我们先定义一个与上一节相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化他的参数。并做一次前向计算。与之前不同的是，在这里我们从nn中导入了init模块，他包含了多种模型初始化的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=3, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "tensor(-0.9908, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "net = nn.Sequential(nn.Linear(4,3),nn.ReLU(),nn.Linear(3,1)) #pytorch已进行默认初始化\n",
    "print(net)\n",
    "X = torch.rand(2,4)\n",
    "Y = net(X).sum()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 访问模型参数\n",
    "#### 回忆一下上一节提到的Sequential类与Module类的继承关系。对于Sequential实例中含模型参数的层，我们可以通过Module类的parameters()或者named_paremeters方法来访问所以参数（以迭代器的形式返回），后者除了返回参数tensor外还会返回其名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n",
      "0.weight torch.Size([3, 4])\n",
      "0.bias torch.Size([3])\n",
      "2.weight torch.Size([1, 3])\n",
      "2.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(type(net.named_parameters()))\n",
    "for name,param in net.named_parameters():\n",
    "    print(name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可见返回的名字自动加上了层数的索引作为前缀。我们再来访问net中单层的参数。对于使用Sequential类构造的神经网络，我们可以通过[]来访问网络的任一层。索引0表示隐藏层为Sequential实例最先添加的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>\n",
      "bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "for name,param in net[0].named_parameters():\n",
    "    print(name,param.size(),type(param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 因为这里是单层所以没有了层数索引的前缀。另外返回的param的类型为torch.nn.Parameter.Parameter 其实这是Tensor的子类，和tensor不同的是如果一个tensor是parameter,那么他会自动被添加进到模型的参数列表里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight1\n"
     ]
    }
   ],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyModule,self).__init__(**kwargs)\n",
    "        self.weight1 = nn.Parameter(torch.rand(20,20))\n",
    "        self.weight2 = torch.rand(20,20)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pass\n",
    "    \n",
    "n = MyModule()\n",
    "for name,param in n.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上面的代码中weight1在参数列表但是weight2却没有在参数列表\n",
    "#### 因为Parameter是Tensor 即Tensor拥有的属性他都有，比如可以根据data来访问参数数值，用grad来访问参数梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4778,  0.4306,  0.3816,  0.1531],\n",
      "        [-0.2253,  0.3370, -0.1947, -0.2327],\n",
      "        [ 0.2025,  0.4766,  0.3331,  0.4300]])\n",
      "None\n",
      "tensor([[ 0.0551,  0.1262,  0.2860,  0.2496],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1057, -0.2422, -0.5490, -0.4791]])\n"
     ]
    }
   ],
   "source": [
    "weight_0 = list(net[0].parameters())[0]\n",
    "print(weight_0.data)\n",
    "print(weight_0.grad)\n",
    "Y.backward()\n",
    "print(weight_0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化 参数模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们在（数值稳定性和模型初始化）中提到了pytorch中，nn.Module的模块参数都采取了较为合理的初始化策略（不同类型的layer具体采样的哪一种初始化方法可以参考源代码）但我们经常需要使用其他方法来初始化权重。pytorch的init模块里提供了多种预设的初始化方法。在下面的例子中，我们将权重参数初始化成均值为0，标准差为0.01的正态分布随机数，并依然将偏差参数清零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[ 5.5000e-03, -2.7738e-03,  5.8352e-03,  4.7030e-03],\n",
      "        [ 4.5872e-05,  3.9806e-03,  3.9415e-03,  3.1600e-04],\n",
      "        [-1.7860e-02,  7.5714e-03, -9.8047e-03,  1.1337e-02]])\n",
      "2.weight tensor([[0.0017, 0.0131, 0.0109]])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init.normal_(param,mean = 0,std = 0.01)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 下面使用常数来初始化权重参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([0., 0., 0.])\n",
      "2.bias tensor([0.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-e57ee7a68b8e>:3: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(param,val =0)\n"
     ]
    }
   ],
   "source": [
    "for name ,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        init.constant(param,val =0)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义初始化方法\n",
    "#### 有时候我们需要的初始化方法并没有在init模块中提供。这时，可以实现一个初始化方法，从而能够像使用其他初始化方法那样使用它在这之前我们先看看pytorch是怎么实现这些初始化方法的，例如torch.nn.init.normal_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_(tensor,mean=0,std=1):\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(mean,std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可以看到这就是一个inplace改变Tensor值的函数，而且这个过程是不记录梯度的。类似的我们来实现一个自定义的初始化方法。在下面的例子里，我们令权重有一半概率初始化为0，有另一半概率初始化为【-10，-5】和【5，10】两个区间里均匀分布的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight tensor([[-7.2898,  7.2934, -0.0000,  0.0000],\n",
      "        [ 5.4258, -0.0000,  0.0000, -0.0000],\n",
      "        [ 0.0000,  5.7859,  0.0000,  6.5694]])\n",
      "2.weight tensor([[-7.6644,  5.9442,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "def init_weight_(tensor):\n",
    "    with torch.no_grad():\n",
    "        tensor.uniform_(-10,10)\n",
    "        tensor *= (tensor.abs() >= 5).float() #为什么可以有一半？\n",
    "for name,param in net.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        init_weight_(param)\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([False, False])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2)\n",
    "print(x)\n",
    "x.abs() > 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 此外，我们还可以通过改变这些参数的data来改写模型参数值同时不会影响梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.bias tensor([1., 1., 1.])\n",
      "2.bias tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param.data += 1\n",
    "        print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共享模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在有些情况下，我们希望在多个层之间共享模型参数。Module类的forward函数里多次调用同一个层。此外，如果我们传入Sequential的模块是同一个Module实例的话参数也是共享的，下面来看一个例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=1, bias=False)\n",
      "  (1): Linear(in_features=1, out_features=1, bias=False)\n",
      ")\n",
      "0.weight tensor([[3.]])\n"
     ]
    }
   ],
   "source": [
    "linear = nn.Linear(1,1,bias = False)\n",
    "net = nn.Sequential(linear,linear)\n",
    "print(net)\n",
    "for name,param in net.named_parameters():\n",
    "    init.constant_(param,val=3)\n",
    "    print(name,param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在内存中，这两个线性层其实一个对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(id(net[0]) == id(net[1]))\n",
    "print(id(net[0].weight) == id(net[1].weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 因为模型里包含了梯度，所以在反向传播计算时，这些共享的参数的梯度是累加的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., grad_fn=<SumBackward0>)\n",
      "tensor([[6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,1)\n",
    "y = net(x).sum()\n",
    "print(y)\n",
    "y.backward()\n",
    "print(net[0].weight.grad)#单次梯度是3，两次所以就是6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小结\n",
    "#### 可以有多种方法来访问，初始化和共享模型参数\n",
    "#### 可以自定义初始化方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
